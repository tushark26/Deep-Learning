{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrw0jVsRrFl5+pZ9M1zpPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushark26/Deep-Learning/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyJ2HnOb9Q6N"
      },
      "source": [
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import IPython\n",
        "from td_utils import *\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "IPython.display.Audio(\"./raw_data/activates/1.wav\")\n",
        "IPython.display.Audio(\"./raw_data/negatives/4.wav\")\n",
        "IPython.display.Audio(\"./raw_data/backgrounds/1.wav\")\n",
        "IPython.display.Audio(\"audio_examples/example_train.wav\")\n",
        "\n",
        "x = graph_spectrogram(\"audio_examples/example_train.wav\")\n",
        "\n",
        "\n",
        "_, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
        "print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
        "print(\"Time steps in input after spectrogram\", x.shape)\n",
        "\n",
        "\n",
        "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
        "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram\n",
        "\n",
        "\n",
        "\n",
        "Ty = 1375 # The number of time steps in the output of our model\n",
        "\n",
        "# Load audio segments using pydub \n",
        "activates, negatives, backgrounds = load_raw_audio()\n",
        "\n",
        "print(\"background len should be 10,000, since it is a 10 sec clip\\n\" + str(len(backgrounds[0])),\"\\n\")\n",
        "print(\"activate[0] len may be around 1000, since an `activate` audio clip is usually around 1 second (but varies a lot) \\n\" + str(len(activates[0])),\"\\n\")\n",
        "print(\"activate[1] len: different `activate` clips can have different lengths\\n\" + str(len(activates[1])),\"\\n\")\n",
        "\n",
        "def get_random_time_segment(segment_ms):\n",
        "    \"\"\"\n",
        "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
        "    \n",
        "    Arguments:\n",
        "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
        "    \n",
        "    Returns:\n",
        "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
        "    \"\"\"\n",
        "    \n",
        "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   # Make sure segment doesn't run past the 10sec background \n",
        "    segment_end = segment_start + segment_ms - 1\n",
        "    \n",
        "    return (segment_start, segment_end)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_overlapping(segment_time, previous_segments):\n",
        "    \"\"\"\n",
        "    Checks if the time of a segment overlaps with the times of existing segments.\n",
        "    \n",
        "    Arguments:\n",
        "    segment_time -- a tuple of (segment_start, segment_end) for the new segment\n",
        "    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\n",
        "    \n",
        "    Returns:\n",
        "    True if the time segment overlaps with any of the existing segments, False otherwise\n",
        "    \"\"\"\n",
        "    \n",
        "    segment_start, segment_end = segment_time\n",
        "    \n",
        "    # Step 1: Initialize overlap as a \"False\" flag. (≈ 1 line)\n",
        "    overlap = False\n",
        "    \n",
        "    # Step 2: loop over the previous_segments start and end times.\n",
        "    # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)\n",
        "    for previous_start, previous_end in previous_segments:\n",
        "        if segment_start <= previous_end and segment_end >= previous_start:\n",
        "            overlap = True\n",
        "\n",
        "    return overlap\n",
        "\n",
        "\n",
        "\n",
        "overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\n",
        "overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\n",
        "print(\"Overlap 1 = \", overlap1)\n",
        "print(\"Overlap 2 = \", overlap2)\n",
        "\n",
        "\n",
        "def insert_audio_clip(background, audio_clip, previous_segments):\n",
        "    \"\"\"\n",
        "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
        "    audio segment does not overlap with existing segments.\n",
        "    \n",
        "    Arguments:\n",
        "    background -- a 10 second background audio recording.  \n",
        "    audio_clip -- the audio clip to be inserted/overlaid. \n",
        "    previous_segments -- times where audio segments have already been placed\n",
        "    \n",
        "    Returns:\n",
        "    new_background -- the updated background audio\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the duration of the audio clip in ms\n",
        "    segment_ms = len(audio_clip)\n",
        "    \n",
        "    # Step 1: Use one of the helper functions to pick a random time segment onto which to insert \n",
        "    # the new audio clip. (≈ 1 line)\n",
        "    segment_time = get_random_time_segment(segment_ms)\n",
        "    \n",
        "    # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep \n",
        "    # picking new segment_time at random until it doesn't overlap. (≈ 2 lines)\n",
        "    while is_overlapping(segment_time, previous_segments):\n",
        "        segment_time = get_random_time_segment(segment_ms)\n",
        "\n",
        "    # Step 3: Append the new segment_time to the list of previous_segments (≈ 1 line)\n",
        "    previous_segments.append(segment_time)\n",
        "    \n",
        "    # Step 4: Superpose audio segment and background\n",
        "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
        "    \n",
        "    return new_background, segment_time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(5)\n",
        "audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])\n",
        "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
        "print(\"Segment Time: \", segment_time)\n",
        "IPython.display.Audio(\"insert_test.wav\")\n",
        "\n",
        "\n",
        "# Expected audio\n",
        "IPython.display.Audio(\"audio_examples/insert_reference.wav\")\n",
        "\n",
        "\n",
        "def insert_ones(y, segment_end_ms):\n",
        "    \"\"\"\n",
        "    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment \n",
        "    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n",
        "    50 following labels should be ones.\n",
        "    \n",
        "    \n",
        "    Arguments:\n",
        "    y -- numpy array of shape (1, Ty), the labels of the training example\n",
        "    segment_end_ms -- the end time of the segment in ms\n",
        "    \n",
        "    Returns:\n",
        "    y -- updated labels\n",
        "    \"\"\"\n",
        "    \n",
        "    # duration of the background (in terms of spectrogram time-steps)\n",
        "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
        "    \n",
        "    # Add 1 to the correct index in the background label (y)\n",
        "    for i in range(segment_end_y + 1, segment_end_y + 51):\n",
        "        if i < Ty:\n",
        "            y[0, i] = 1\n",
        "    \n",
        "    return y\n",
        "\n",
        "\n",
        "arr1 = insert_ones(np.zeros((1, Ty)), 9700)\n",
        "plt.plot(insert_ones(arr1, 4251)[0,:])\n",
        "print(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635])\n",
        "\n",
        "\n",
        "def create_training_example(background, activates, negatives):\n",
        "    \"\"\"\n",
        "    Creates a training example with a given background, activates, and negatives.\n",
        "    \n",
        "    Arguments:\n",
        "    background -- a 10 second background audio recording\n",
        "    activates -- a list of audio segments of the word \"activate\"\n",
        "    negatives -- a list of audio segments of random words that are not \"activate\"\n",
        "    \n",
        "    Returns:\n",
        "    x -- the spectrogram of the training example\n",
        "    y -- the label at each time step of the spectrogram\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set the random seed\n",
        "    np.random.seed(18)\n",
        "    \n",
        "    # Make background quieter\n",
        "    background = background - 20\n",
        "\n",
        "\n",
        "    # Step 1: Initialize y (label vector) of zeros (≈ 1 line)\n",
        "    y = np.zeros((1, Ty))\n",
        "\n",
        "    # Step 2: Initialize segment times as an empty list (≈ 1 line)\n",
        "    previous_segments = []\n",
        "\n",
        "    \n",
        "    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n",
        "    number_of_activates = np.random.randint(0, 5)\n",
        "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
        "    random_activates = [activates[i] for i in random_indices]\n",
        "    \n",
        "    # Step 3: Loop over randomly selected \"activate\" clips and insert in background\n",
        "    for random_activate in random_activates:\n",
        "        # Insert the audio clip on the background\n",
        "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
        "        # Retrieve segment_start and segment_end from segment_time\n",
        "        segment_start, segment_end = segment_time\n",
        "        # Insert labels in \"y\"\n",
        "        y = insert_ones(y, segment_end_ms=segment_end)\n",
        "\n",
        "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
        "    number_of_negatives = np.random.randint(0, 3)\n",
        "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
        "    random_negatives = [negatives[i] for i in random_indices]\n",
        "\n",
        "    # Step 4: Loop over randomly selected negative clips and insert in background\n",
        "    for random_negative in random_negatives:\n",
        "        # Insert the audio clip on the background \n",
        "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
        "\n",
        "    \n",
        "    # Standardize the volume of the audio clip \n",
        "    background = match_target_amplitude(background, -20.0)\n",
        "\n",
        "    # Export new training example \n",
        "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
        "    print(\"File (train.wav) was saved in your directory.\")\n",
        "    \n",
        "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
        "    x = graph_spectrogram(\"train.wav\")\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x, y = create_training_example(backgrounds[0], activates, negatives)\n",
        "\n",
        "\n",
        "\n",
        "IPython.display.Audio(\"train.wav\")\n",
        "IPython.display.Audio(\"audio_examples/train_reference.wav\")\n",
        "plt.plot(y[0])\n",
        "\n",
        "# Load preprocessed training examples\n",
        "X = np.load(\"./XY_train/X.npy\")\n",
        "Y = np.load(\"./XY_train/Y.npy\")\n",
        "\n",
        "\n",
        "# Load preprocessed dev set examples\n",
        "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
        "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")\n",
        "\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
        "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "def model(input_shape):\n",
        "    \"\"\"\n",
        "    Function creating the model's graph in Keras.\n",
        "    \n",
        "    Argument:\n",
        "    input_shape -- shape of the model's input data (using Keras conventions)\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(shape = input_shape)\n",
        "    \n",
        "\n",
        "    \n",
        "    # Step 1: CONV layer (≈4 lines)\n",
        "    X = Conv1D(196, 15, strides=4)(X_input)                                  # CONV1D\n",
        "    X = BatchNormalization()(X)                                  # Batch normalization\n",
        "    X = Activation('relu')(X)                                   # ReLu activation\n",
        "    X = Dropout(0.8)(X)                                  # dropout (use 0.8)\n",
        "\n",
        "    # Step 2: First GRU Layer (≈4 lines)\n",
        "    X = GRU(units = 128, return_sequences=True)(X)                                 # GRU (use 128 units and return the sequences)\n",
        "    X = Dropout(0.8)(X)                                  # dropout (use 0.8)\n",
        "    X = BatchNormalization()(X)                                 # Batch normalization\n",
        "    \n",
        "    # Step 3: Second GRU Layer (≈4 lines)\n",
        "    X = GRU(units = 128, return_sequences=True)(X)      # GRU (use 128 units and return the sequences)\n",
        "    X = Dropout(0.8)(X)                                  # dropout (use 0.8)\n",
        "    X = BatchNormalization()(X)                                 # Batch normalization\n",
        "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
        "    \n",
        "    # Step 4: Time-distributed dense layer (see given code in instructions) (≈1 line)\n",
        "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
        "\n",
        "\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "    \n",
        "    return model  \n",
        "\n",
        "\n",
        "\n",
        "model = model(input_shape = (Tx, n_freq))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model = load_model('./models/tr_model.h5')\n",
        "\n",
        "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "model.fit(X, Y, batch_size = 5, epochs=1)\n",
        "\n",
        "\n",
        "loss, acc = model.evaluate(X_dev, Y_dev)\n",
        "print(\"Dev set accuracy = \", acc)\n",
        "\n",
        "\n",
        "def detect_triggerword(filename):\n",
        "    plt.subplot(2, 1, 1)\n",
        "\n",
        "    x = graph_spectrogram(filename)\n",
        "    # the spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
        "    x  = x.swapaxes(0,1)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    predictions = model.predict(x)\n",
        "    \n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(predictions[0,:,0])\n",
        "    plt.ylabel('probability')\n",
        "    plt.show()\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "chime_file = \"audio_examples/chime.wav\"\n",
        "def chime_on_activate(filename, predictions, threshold):\n",
        "    audio_clip = AudioSegment.from_wav(filename)\n",
        "    chime = AudioSegment.from_wav(chime_file)\n",
        "    Ty = predictions.shape[1]\n",
        "    # Step 1: Initialize the number of consecutive output steps to 0\n",
        "    consecutive_timesteps = 0\n",
        "    # Step 2: Loop over the output steps in the y\n",
        "    for i in range(Ty):\n",
        "        # Step 3: Increment consecutive output steps\n",
        "        consecutive_timesteps += 1\n",
        "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
        "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
        "            # Step 5: Superpose audio and background using pydub\n",
        "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
        "            # Step 6: Reset consecutive output steps to 0\n",
        "            consecutive_timesteps = 0\n",
        "        \n",
        "    audio_clip.export(\"chime_output.wav\", format='wav')\n",
        "\n",
        "\n",
        "\n",
        "IPython.display.Audio(\"./raw_data/dev/1.wav\")\n",
        "\n",
        "\n",
        "IPython.display.Audio(\"./raw_data/dev/2.wav\")\n",
        "\n",
        "\n",
        "filename = \"./raw_data/dev/1.wav\"\n",
        "prediction = detect_triggerword(filename)\n",
        "chime_on_activate(filename, prediction, 0.5)\n",
        "IPython.display.Audio(\"./chime_output.wav\")\n",
        "\n",
        "\n",
        "filename  = \"./raw_data/dev/2.wav\"\n",
        "prediction = detect_triggerword(filename)\n",
        "chime_on_activate(filename, prediction, 0.5)\n",
        "IPython.display.Audio(\"./chime_output.wav\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}